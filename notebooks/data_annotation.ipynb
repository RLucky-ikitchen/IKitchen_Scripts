{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QX3ZiD22Uo1",
        "outputId": "e4f516a0-1c14-408e-d590-f950b03e7b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XulDpZRLr-2k",
        "outputId": "f5701bec-d7d7-4a1d-f0f4-85972e3753b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.37.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.37.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.37.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=\"sk-proj-EhMwa5EECVtbpgtbTLroT3BlbkFJWTqh2jvIkszX28jlydCA\")"
      ],
      "metadata": {
        "id": "Vaa1E6W8scX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.chat.completions.create(\n",
        "messages=[{\"role\": \"user\", \"content\":\"hello\"}],\n",
        "model=\"gpt-4o\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgSs9ozZscaM",
        "outputId": "b6a75fc1-39e1-4b7e-92d8-1ef8b8150e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9rKGgQIqSrZNzBN7AlyQ1sLiQN5Z4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1722496322, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_4e2b2da518', usage=CompletionUsage(completion_tokens=9, prompt_tokens=8, total_tokens=17))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "source": [
        "import openai\n",
        "import csv\n",
        "import json\n",
        "\n",
        "# OpenAI API key\n",
        "openai.api_key = 'sk-proj-EhMwa5EECVtbpgtbTLroT3BlbkFJWTqh2jvIkszX28jlydCA'\n",
        "\n",
        "# File paths\n",
        "input_csv_file = '/content/drive/MyDrive/WhatApp Chat/New chats/cleaned_data.csv'\n",
        "output_json_file = 'categorized_conversations.json'\n",
        "output_csv_file = 'categorized_conversations.csv'\n",
        "\n",
        "# List of possible categories\n",
        "query_categories = [\n",
        "    \"Opening Hours\",\n",
        "    \"Address\",\n",
        "    \"Menu\",\n",
        "    \"Complaint\",\n",
        "    \"Reservation\",\n",
        "    \"Incomplete Query\",\n",
        "    \"Place Order\",\n",
        "    \"Partnerships and Collaborations\",\n",
        "    \"Portion Size Inquiry\",\n",
        "    \"Delivery Status\",\n",
        "    \"Payment Issues\"\n",
        "]\n",
        "\n",
        "support_response_categories = [\n",
        "    \"Good\",\n",
        "    \"Bad\",\n",
        "    \"No Answer\"\n",
        "]\n",
        "\n",
        "# Read conversations from CSV\n",
        "conversations = []\n",
        "with open(input_csv_file, newline='', encoding='utf-8') as csvfile:\n",
        "    csv_reader = csv.reader(csvfile)\n",
        "    header = next(csv_reader)\n",
        "    for row in csv_reader:\n",
        "        conversations.append({\"id\": row[0], \"conversation\": row[1]})  # Adjust indices as needed\n",
        "\n",
        "categorized_conversations = []\n",
        "\n",
        "for conv in conversations:\n",
        "    conversation = conv['conversation']\n",
        "\n",
        "    # Split the conversation into smaller chunks\n",
        "    chunk_size = 2000  # Adjust this value based on the average length of your conversations\n",
        "    conversation_chunks = [conversation[i:i+chunk_size] for i in range(0, len(conversation), chunk_size)]\n",
        "\n",
        "    categories_json = {\"query_categories\": [], \"support_response_categories\": []}\n",
        "\n",
        "    for chunk in conversation_chunks:\n",
        "        prompt = {\n",
        "            \"instruction\": \"You will be provided with a part of a conversation and a list of possible query categories and support response categories. Please analyze this part of the conversation and select the most relevant categories for both the query and the support response. You can select multiple query categories if applicable. Return the output in JSON format as specified below.\",\n",
        "            \"conversation\": chunk,\n",
        "            \"query_categories\": query_categories,\n",
        "            \"support_response_categories\": support_response_categories,\n",
        "            \"output_format\": {\n",
        "                \"query_categories\": [],\n",
        "                \"support_response_categories\": []\n",
        "            }\n",
        "        }\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an assistant helping to categorize conversations.\"},\n",
        "                {\"role\": \"user\", \"content\": json.dumps(prompt)}\n",
        "            ],\n",
        "            temperature=0,\n",
        "            n=1,\n",
        "            stop=None\n",
        "            #json_mode=True\n",
        "        )\n",
        "\n",
        "        # Accumulate categories from each chunk\n",
        "        chunk_categories = json.loads(response.choices[0].message.content)\n",
        "        categories_json['query_categories'].extend(chunk_categories['query_categories'])\n",
        "        categories_json['support_response_categories'].extend(chunk_categories['support_response_categories'])\n",
        "\n",
        "    # Remove duplicates from accumulated categories\n",
        "    categories_json['query_categories'] = list(set(categories_json['query_categories']))\n",
        "    categories_json['support_response_categories'] = list(set(categories_json['support_response_categories']))\n",
        "\n",
        "    categorized_conversations.append({\n",
        "        \"id\": conv['id'],\n",
        "        \"conversation\": conversation,\n",
        "        \"query_categories\": categories_json['query_categories'],\n",
        "        \"support_response_categories\": categories_json['support_response_categories']\n",
        "    })\n",
        "\n",
        "# Save to JSON file\n",
        "with open(output_json_file, 'w') as jsonfile:\n",
        "    json.dump(categorized_conversations, jsonfile, indent=4)\n",
        "\n",
        "# Save to CSV file\n",
        "with open(output_csv_file, 'w', newline='') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow([\"ID\", \"Conversation\", \"Query Categories\", \"Support Response Categories\"])\n",
        "    for conv in categorized_conversations:\n",
        "        csv_writer.writerow([\n",
        "            conv[\"id\"],\n",
        "            conv[\"conversation\"],\n",
        "            \", \".join(conv[\"query_categories\"]),\n",
        "            \", \".join(conv[\"support_response_categories\"])\n",
        "        ])\n",
        "\n",
        "print(\"Categorization complete and saved to JSON and CSV files.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BFs-Yy4Z9WBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Replace 'your-api-key' with your actual OpenAI API key\n",
        "openai.api_key = 'sk-proj-EhMwa5EECVtbpgtbTLroT3BlbkFJWTqh2jvIkszX28jlydCA'\n",
        "\n",
        "def verify_bangla_conversation(messages):\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4\",  # Specify the model as GPT-4\n",
        "            messages=messages,\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "# Example Bangla conversation as messages\n",
        "bangla_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that understands Bangla.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Apnader location kuthay?\"}\n",
        "]\n",
        "\n",
        "# Verify Bangla conversation understanding\n",
        "result = verify_bangla_conversation(bangla_messages)\n",
        "print(\"Response from GPT-4:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPd7qfht9eKm",
        "outputId": "a622e90f-7f6b-41a4-c1ea-929f8abde3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from GPT-4: Ami ekta artificial intelligence, tai amar kono sthan nai. Ami jekono jaigay mora thakte pari jekhane internet connection ache.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1SmOezE9eID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3HBuf8ZW2TIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}